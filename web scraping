import requests
from bs4 import BeautifulSoup

# URL to scrape
url = "http://quotes.toscrape.com"

# Send a GET request
response = requests.get(url)

# Check if the request was successful
if response.status_code == 200:
    # Parse the HTML content
    soup = BeautifulSoup(response.text, 'html.parser')

# Find all quote elements
    quotes = soup.find_all('span', class_='text')
    authors = soup.find_all('small', class_='author')

    # Loop through and print quotes and their authors
    for quote, author in zip(quotes, authors):
        print(f"Quote: {quote.text}")
        print(f"Author: {author.text}\n")
else:
    print(f"Failed to retrieve the webpage. Status code: {response.status_code}")



# 1
# Basic Web Scraping using requests and BeautifulSoup

import requests
from bs4 import BeautifulSoup

# Send an HTTP request to the webpage
url = "https://www.example.com"
response = requests.get(url)

# Parse the HTML content
soup = BeautifulSoup(response.content, "html.parser")

# Extract and print the title of the page
page_title = soup.title.string
print("Page Title:", page_title)

# 2
# Scraping Multiple Elements (e.g., Article Titles)

import requests
from bs4 import BeautifulSoup

url = "https://news.ycombinator.com/"
response = requests.get(url)

# Parse the content of the response
soup = BeautifulSoup(response.content, "html.parser")

# Find all elements with the class 'storylink'
titles = soup.find_all("a", class_="storylink")

# Print each title
for idx, title in enumerate(titles, 1):
    print(f"{idx}. {title.get_text()}")


# 3
# Scraping Data from a Table

import requests
from bs4 import BeautifulSoup

# Find the table by its class or ID
table = soup.find("table", id="example2")

# Loop through the rows of the table
for row in table.find_all("tr"):
    columns = row.find_all("td")
    if columns:  # Ensure the row has data
        country = columns[1].get_text()
        population = columns[2].get_text()
        print(f"Country: {country}, Population: {population}")


# 4
# Using Selenium for Dynamic Pages

from selenium import webdriver
from selenium.webdriver.common.keys import Keys

# Set up the WebDriver (e.g., ChromeDriver)
driver = webdriver.Chrome()

