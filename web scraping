import requests
from bs4 import BeautifulSoup

# URL to scrape
url = "http://quotes.toscrape.com"

# Send a GET request
response = requests.get(url)

# Check if the request was successful
if response.status_code == 200:
    # Parse the HTML content
    soup = BeautifulSoup(response.text, 'html.parser')

# Find all quote elements
    quotes = soup.find_all('span', class_='text')
    authors = soup.find_all('small', class_='author')

    # Loop through and print quotes and their authors
    for quote, author in zip(quotes, authors):
        print(f"Quote: {quote.text}")
        print(f"Author: {author.text}\n")
else:
    print(f"Failed to retrieve the webpage. Status code: {response.status_code}")



# 1
# Basic Web Scraping using requests and BeautifulSoup

import requests
from bs4 import BeautifulSoup

# Send an HTTP request to the webpage
url = "https://www.example.com"
response = requests.get(url)

# Parse the HTML content
soup = BeautifulSoup(response.content, "html.parser")

# Extract and print the title of the page
page_title = soup.title.string
print("Page Title:", page_title)

# 2
# Scraping Multiple Elements (e.g., Article Titles)

import requests
from bs4 import BeautifulSoup

url = "https://news.ycombinator.com/"
response = requests.get(url)

# Parse the content of the response
soup = BeautifulSoup(response.content, "html.parser")

# Find all elements with the class 'storylink'
titles = soup.find_all("a", class_="storylink")

# Print each title
for idx, title in enumerate(titles, 1):
    print(f"{idx}. {title.get_text()}")


# 3
# Scraping Data from a Table

import requests
from bs4 import BeautifulSoup

# Find the table by its class or ID
table = soup.find("table", id="example2")

# Loop through the rows of the table
for row in table.find_all("tr"):
    columns = row.find_all("td")
    if columns:  # Ensure the row has data
        country = columns[1].get_text()
        population = columns[2].get_text()
        print(f"Country: {country}, Population: {population}")


# 4
# Using Selenium for Dynamic Pages

from selenium import webdriver
from selenium.webdriver.common.keys import Keys

# Set up the WebDriver (e.g., ChromeDriver)
driver = webdriver.Chrome()

# Open the website
driver.get("https://www.example.com")

# Locate an element, perform an action, and extract the result
search_box = driver.find_element("name", "q")
search_box.send_keys("Python")
search_box.send_keys(Keys.RETURN)

# Wait for results to load, and extract the data
driver.implicitly_wait(5)
results = driver.find_elements("class name", "result")

for result in results:
    print(result.text)

# Close the browser
driver.quit()

# 5
# Handling Pagination

base_url = "https://example.com/page/"

for page in range(1, 6):  # Iterate through first 5 pages
    response = requests.get(base_url + str(page))
    soup = BeautifulSoup(response.content, "html.parser")

    items = soup.find_all("div", class_="item")
    
    for item in items:
        title = item.find("h2").get_text()
        price = item.find("span", class_="price").get_text()
        print(f"Title: {title}, Price: {price}")


# 6
# Saving Scraped Data to a CSV File

import csv
import requests
from bs4 import BeautifulSoup

url = "https://www.example.com"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

# Open a CSV file to save the scraped data
with open("data.csv", "w", newline="") as csvfile:
    csvwriter = csv.writer(csvfile)
    csvwriter.writerow(["Title", "Price"])  # Writing header row

    items = soup.find_all("div", class_="item")

    for item in items:
        title = item.find("h2").get_text()
        price = item.find("span", class_="price").get_text()
        csvwriter.writerow([title, price])

# 7
# Using Scrapy for Larger Projects

import scrapy

class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        "http://quotes.toscrape.com/",
    ]

    def parse(self, response):
        for quote in response.css("div.quote"):
            yield {
                "text": quote.css("span.text::text").get(),
                "author": quote.css("small.author::text").get(),
                }

        next_page = response.css("li.next a::attr(href)").get()
        if next_page:
            yield response.follow(next_page, self.parse)


# Basic Web Scraping using requests and BeautifulSoup
# 2

# Extracting All Links (<a> Tags)

import requests
from bs4 import BeautifulSoup

url = "https://www.example.com"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

# Find all <a> tags and extract the href attributes (links)
for link in soup.find_all('a'):
    href = link.get('href')
    if href:
        print(href)

# 3
# Extracting Headings (H1, H2, H3, etc.)

import requests
from bs4 import BeautifulSoup

url = "https://www.example.com"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

# Extract and print all heading tags (h1, h2, h3, etc.)
for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
    print(heading.name + ":", heading.get_text())

# 4 
# Extracting Paragraph Texts

import requests
from bs4 import BeautifulSoup

url = "https://www.example.com"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

# Extract and print all paragraph tags
for paragraph in soup.find_all('p'):
    print(paragraph.get_text())

