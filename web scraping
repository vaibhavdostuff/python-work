import requests
from bs4 import BeautifulSoup

# URL to scrape
url = "http://quotes.toscrape.com"

# Send a GET request
response = requests.get(url)

# Check if the request was successful
if response.status_code == 200:
    # Parse the HTML content
    soup = BeautifulSoup(response.text, 'html.parser')

# Find all quote elements
    quotes = soup.find_all('span', class_='text')
    authors = soup.find_all('small', class_='author')

    # Loop through and print quotes and their authors
    for quote, author in zip(quotes, authors):
        print(f"Quote: {quote.text}")
        print(f"Author: {author.text}\n")
else:
    print(f"Failed to retrieve the webpage. Status code: {response.status_code}")



# 1
# Basic Web Scraping using requests and BeautifulSoup

import requests
from bs4 import BeautifulSoup

# Send an HTTP request to the webpage
url = "https://www.example.com"
response = requests.get(url)

# Parse the HTML content
soup = BeautifulSoup(response.content, "html.parser")

# Extract and print the title of the page
page_title = soup.title.string
print("Page Title:", page_title)

# 2
# Scraping Multiple Elements (e.g., Article Titles)

import requests
from bs4 import BeautifulSoup

url = "https://news.ycombinator.com/"
response = requests.get(url)

# Parse the content of the response
soup = BeautifulSoup(response.content, "html.parser")

# Find all elements with the class 'storylink'
titles = soup.find_all("a", class_="storylink")

# Print each title
for idx, title in enumerate(titles, 1):
    print(f"{idx}. {title.get_text()}")


# 3
# Scraping Data from a Table

import requests
from bs4 import BeautifulSoup

# Find the table by its class or ID
table = soup.find("table", id="example2")

# Loop through the rows of the table
for row in table.find_all("tr"):
    columns = row.find_all("td")
    if columns:  # Ensure the row has data
        country = columns[1].get_text()
        population = columns[2].get_text()
        print(f"Country: {country}, Population: {population}")


# 4
# Using Selenium for Dynamic Pages

from selenium import webdriver
from selenium.webdriver.common.keys import Keys

# Set up the WebDriver (e.g., ChromeDriver)
driver = webdriver.Chrome()

# Open the website
driver.get("https://www.example.com")

# Locate an element, perform an action, and extract the result
search_box = driver.find_element("name", "q")
search_box.send_keys("Python")
search_box.send_keys(Keys.RETURN)

# Wait for results to load, and extract the data
driver.implicitly_wait(5)
results = driver.find_elements("class name", "result")

for result in results:
    print(result.text)

# Close the browser
driver.quit()

# 5
# Handling Pagination

base_url = "https://example.com/page/"

for page in range(1, 6):  # Iterate through first 5 pages
    response = requests.get(base_url + str(page))
    soup = BeautifulSoup(response.content, "html.parser")

    items = soup.find_all("div", class_="item")
    
    for item in items:
        title = item.find("h2").get_text()
        price = item.find("span", class_="price").get_text()
        print(f"Title: {title}, Price: {price}")


# 6
# Saving Scraped Data to a CSV File

import csv
import requests
from bs4 import BeautifulSoup

url = "https://www.example.com"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

# Open a CSV file to save the scraped data
with open("data.csv", "w", newline="") as csvfile:
    csvwriter = csv.writer(csvfile)
    csvwriter.writerow(["Title", "Price"])  # Writing header row

    items = soup.find_all("div", class_="item")

    for item in items:
        title = item.find("h2").get_text()
        price = item.find("span", class_="price").get_text()
        csvwriter.writerow([title, price])

# 7
# Using Scrapy for Larger Projects

import scrapy

class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        "http://quotes.toscrape.com/",
    ]

    def parse(self, response):
        for quote in response.css("div.quote"):
            yield {
                "text": quote.css("span.text::text").get(),
                "author": quote.css("small.author::text").get(),
                }

        next_page = response.css("li.next a::attr(href)").get()
        if next_page:
            yield response.follow(next_page, self.parse)


# Basic Web Scraping using requests and BeautifulSoup
# 2

# Extracting All Links (<a> Tags)

import requests
from bs4 import BeautifulSoup

url = "https://www.example.com"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

# Find all <a> tags and extract the href attributes (links)
for link in soup.find_all('a'):
    href = link.get('href')
    if href:
        print(href)

# 3
# Extracting Headings (H1, H2, H3, etc.)

import requests
from bs4 import BeautifulSoup

url = "https://www.example.com"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

# Extract and print all heading tags (h1, h2, h3, etc.)
for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
    print(heading.name + ":", heading.get_text())

# 4 
# Extracting Paragraph Texts

import requests
from bs4 import BeautifulSoup

url = "https://www.example.com"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

# Extract and print all paragraph tags
for paragraph in soup.find_all('p'):
    print(paragraph.get_text())

# 5
# Extracting Image URLs

import requests
from bs4 import BeautifulSoup

url = "https://www.example.com"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

# Find all <img> tags and print the source (src) URLs
for img in soup.find_all('img'):
    print(img.get('src'))

# 6 
# Extracting Data from a Table

import requests
from bs4 import BeautifulSoup

url = "https://www.example.com/tablepage"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

# Find the table element
table = soup.find("table")

# Iterate over the rows in the table and extract data
for row in table.find_all("tr"):
    cells = row.find_all("td")
    if cells:
        data = [cell.get_text() for cell in cells]
        print(data)


# 6 
# Extracting Meta Tags (SEO Data)

import requests
from bs4 import BeautifulSoup

url = "https://www.example.com"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

# Extract and print meta tag content
for meta in soup.find_all('meta'):
    print(meta.get('name'), meta.get('content'))

# 7
# Extracting Data by Specific Class or ID

import requests
from bs4 import BeautifulSoup

url = "https://www.example.com"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

# Find and print an element with a specific class
content = soup.find('div', class_='specific-class-name')
if content:
    print(content.get_text())

# Find and print an element by ID
element = soup.find(id='specific-id')
if element:
    print(element.get_text())


# 8 
# Extracting Attributes of Elements

import requests
from bs4 import BeautifulSoup

url = "https://www.example.com"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

# Extract and print 'alt' attributes from images
for img in soup.find_all('img'):
    alt_text = img.get('alt')
    print(f"Alt Text: {alt_text}")

 # 9
 # Extracting and Saving a Webpage's Data to a File

import requests
from bs4 import BeautifulSoup

url = "https://www.example.com"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

# Write the text content of all paragraphs to a file
with open("webpage_content.txt", "w") as file:
    for paragraph in soup.find_all('p'):
        file.write(paragraph.get_text() + "\n")

# 10
# Handling Pagination (Multi-Page Scraping)

import requests
from bs4 import BeautifulSoup

base_url = "https://example.com/page/"
page_num = 1

while True:
    url = base_url + str(page_num)
    response = requests.get(url)
    
    # Stop if no more pages (or page doesn't exist)
    if response.status_code != 200:
        break

    soup = BeautifulSoup(response.content, "html.parser")

    # Extract data (e.g., titles) from the current page
    titles = soup.find_all("h2", class_="title")
    for title in titles:
        print(title.get_text())

    # Move to the next page
    page_num += 1

# 11
# Scraping News Headlines from a News Website

import requests
from bs4 import BeautifulSoup

url = "https://www.example-news.com"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

# Assuming the headlines are inside <h3> tags with a class 'headline'
for headline in soup.find_all("h3", class_="headline"):
    print(headline.get_text().strip())


# 12
# Scraping Product Names from an E-Commerce Website

import requests
from bs4 import BeautifulSoup

url = "https://www.example-store.com/search?q=laptops"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

# Assuming the product names are in <span> tags with a class 'product-name'
for product in soup.find_all("span", class_="product-name"):
    print(product.get_text().strip())

# 13
# Scraping Job Listings from a Job Board

import requests
from bs4 import BeautifulSoup

url = "https://www.example-jobs.com/jobs"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

# Assuming job titles are in <h2> tags with a class 'job-title'
for job in soup.find_all("h2", class_="job-title"):
    print(job.get_text().strip())


# 14
# Scraping Blog Post Titles and URLs

import requests
from bs4 import BeautifulSoup

url = "https://example-blog.com"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

# Find all article links (assuming they are inside <a> tags with class 'article-link')
for link in soup.find_all("a", class_="article-link"):
    title = link.get_text().strip()
    url = link.get('href')
    print(f"Title: {title}, URL: {url}")

# 15
# Scraping Multiple Data Points from an Article (Title, Author, Date)

import requests
from bs4 import BeautifulSoup

url = "https://example-news.com"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

# Find all articles
for article in soup.find_all("div", class_="article"):
    title = article.find("h2", class_="title").get_text().strip()
    author = article.find("span", class_="author").get_text().strip()
    date = article.find("time", class_="date").get_text().strip()
    print(f"Title: {title}, Author: {author}, Date: {date}")


# 16
# Scraping Titles from Multiple Pages (Handling Pagination)

import requests
from bs4 import BeautifulSoup

base_url = "https://example-blog.com/page/"
page_number = 1

while True:
    url = base_url + str(page_number)
    response = requests.get(url)
    
    # Break if no more pages
    if response.status_code != 200:
        break

    soup = BeautifulSoup(response.content, "html.parser")

    # Scrape titles on the current page
    titles = soup.find_all("h2", class_="post-title")
    if not titles:
        break

    for title in titles:
        print(title.get_text().strip())

    # Move to the next page
    page_number += 1

# 17
# Scraping Event Titles and Dates from an Events Page

import requests
from bs4 import BeautifulSoup

url = "https://example-events.com"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

# Find all events
for event in soup.find_all("div", class_="event-item"):
    title = event.find("h3", class_="event-title").get_text().strip()
    date = event.find("span", class_="event-date").get_text().strip()
    print(f"Event: {title} on {date}")

# 18
# Scraping Blog Titles and Saving Them to a List

import requests
from bs4 import BeautifulSoup

url = "https://example-blog.com"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

# Store titles in a list
titles = [title.get_text().strip() for title in soup.find_all("h2", class_="post-title")]

# Print the list of titles
print("Blog Titles:")
for t in titles:
    print(t)


# 1
# Saving Scraped Data to a CSV File (Multiple Columns)

import csv
import requests
from bs4 import BeautifulSoup

url = "https://example-ecommerce.com"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

# Open a CSV file to save the scraped data
with open("products.csv", "w", newline="") as csvfile:
     csvwriter = csv.writer(csvfile)
     csvwriter.writerow(["Product Name", "Price", "Rating"])  # Writing header row

     products = soup.find_all("div", class_="product-item")

     for product in products:
        name = product.find("h3", class_="product-name").get_text()
        price = product.find("span", class_="price").get_text()
        rating = product.find("span", class_="rating").get_text()
        csvwriter.writerow([name, price, rating])


# 2
#  Saving Scraped Data to a JSON File

import json
import requests
from bs4 import BeautifulSoup

url = "https://example-articles.com"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

articles_data = []

articles = soup.find_all("article")
for article in articles:
    title = article.find("h2", class_="title").get_text()
    author = article.find("span", class_="author").get_text()
    date = article.find("time", class_="date").get_text()
    articles_data.append({"Title": title, "Author": author, "Date": date})


# Save to a JSON file
with open("articles.json", "w") as jsonfile:
    json.dump(articles_data, jsonfile, indent=4)


# 3
# Saving Scraped Data to an Excel File

import requests
from bs4 import BeautifulSoup
import openpyxl


url = "https://example-books.com"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

# Create a new Excel workbook and sheet
workbook = openpyxl.Workbook()
sheet = workbook.active
sheet.title = "Books"
sheet.append(["Title", "Author", "Price"])  # Adding headers

# Scrape data and write to the Excel sheet
books = soup.find_all("div", class_="book")
for book in books:
    title = book.find("h3", class_="book-title").get_text()
    author = book.find("span", class_="book-author").get_text()
    price = book.find("span", class_="book-price").get_text()
    sheet.append([title, author, price])

# Save the workbook to a file
workbook.save("books.xlsx")

# 4
# Saving Scraped Data to a Pandas DataFrame

import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://example-articles.com"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

# Create a list to store data
data = []

# Scrape the data
articles = soup.find_all("div", class_="article")
for article in articles:
    title = article.find("h2", class_="title").get_text()
    author = article.find("span", class_="author").get_text()
    date = article.find("time", class_="date").get_text()
    data.append([title, author, date])

# Convert the list to a DataFrame
df = pd.DataFrame(data, columns=["Title", "Author", "Date"])

# Export the DataFrame to a CSV file
df.to_csv("articles.csv", index=False)

# 5
# Saving Data with Date-Time Stamp in the File Name

import csv
import requests
from bs4 import BeautifulSoup
from datetime import datetime

url = "https://example-shop.com"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

# Get the current date and time
current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
filename = f"products_{current_time}.csv"

# Open a CSV file to save the scraped data
with open(filename, "w", newline="") as csvfile:
    csvwriter = csv.writer(csvfile)
    csvwriter.writerow(["Product Name", "Price"])  # Writing header row

    products = soup.find_all("div", class_="product-item")
    for product in products:
        name = product.find("h3", class_="product-name").get_text()
        price = product.find("span", class_="price").get_text()
        csvwriter.writerow([name, price])

print(f"Data saved to {filename}")

# 6
# Handling Missing Data While Scraping

import csv
import requests
from bs4 import BeautifulSoup

url = "https://example-jobs.com"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

with open("jobs.csv", "w", newline="") as csvfile:
    csvwriter = csv.writer(csvfile)
    csvwriter.writerow(["Job Title", "Company", "Location"])  # Writing header row

    jobs = soup.find_all("div", class_="job-listing")
    for job in jobs:
        title = job.find("h2", class_="job-title").get_text() if job.find("h2", class_="job-title") else "N/A"
        company = job.find("span", class_="company").get_text() if job.find("span", class_="company") else "N/A"
        location = job.find("span", class_="location").get_text() if job.find("span", class_="location") else "N/A"
        csvwriter.writerow([title, company, location])


# 1
# Scraping Data with Login

import scrapy

class LoginSpider(scrapy.Spider):
    name = "login_spider"
    start_urls = ["http://quotes.toscrape.com/login"]

    def parse(self, response):
        # Perform login
        return scrapy.FormRequest.from_response(
            response,
            formdata={"username": "your_username", "password": "your_password"},
            callback=self.after_login
        )
    
    def after_login(self, response):
        if b"Logout" not in response.body:
            self.logger.error("Login failed")
            return
        else:
            # Continue scraping quotes
            yield from self.parse_quotes(response)

    
    def parse_quotes(self, response):
        for quote in response.css("div.quote"):
            yield {
                "text": quote.css("span.text::text").get(),
                "author": quote.css("small.author::text").get(),
                "tags": quote.css("div.tags a.tag::text").getall(),